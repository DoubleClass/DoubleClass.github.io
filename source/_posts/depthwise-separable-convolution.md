---
title: depthwise separable convolution
date: 2021-02-01 20:23:28
tags:
- 机器学习
---

## 前记

每次论文读到卷积相关，都会有些知识需要再查一遍，这次总结在这里，以备检查。

## 卷积神经网络

所有内容，这个动图全部可以解决

![](https://i.loli.net/2021/02/06/CguBQNo7DmVkltx.gif)

**方便记忆的几条：**

1. 输出层数 == 卷积核个数
2. 卷积核的深度与输入的深度相同
3. 在每个深度都做一遍相乘，然后相加，再加bias





**卷积的计算方式**

上图描述了一个 **5\*5\*3** 的输入特征图边界采用**1个0像素填充**，利用 **2 个 3\*3 的卷积核**，其**步长为2**进行计算的例子：最后的输出特征图是 **3\*3\*2**。

这个是怎么得到的呢，我先假定一些卷积相关的基本采参数：

输入特征图高：Hi，宽：wi

输入特征图的边界填充像素个数： p

卷积核尺寸(高x宽)：Kh*Kw

卷积核滑动步长：s

输出特征图高：Ho，宽：Wo

则：**Ho = [(Hi - Kh + 2p) / s] + 1 ，Wo = [(wi - Kw + 2p) / s] + 1**

注：这里的 **[x] 表示向下取整**：e.g. [5 / 2] = 2

所以上图中：Ho = [5 - 3 + 2*1] / 2 + 1 = 3 ；Wo = [5 - 3 + 2*p] / 2 + 1 = 3

由于采用了2个3*3的卷积核，所以输出两个特征图，其大小都为3*3

## depthwise-separable 卷积

depthwise-separable 卷积，可以减少参数，或者说，相同参数量，可以做到更深。

首先，使用一个和input channel相同数量的核，每个核负责一个channel

![image-20210206124415386](https://i.loli.net/2021/02/06/KBmNgjSXfUW4nuq.png)

这样输出和输入的channel完全相同，不能扩展feature，所以还有下一步，用一个1×1×M的卷积核，来再做一遍卷积，有几个核，就有几个输出feature map，虽然变成两步，但是参数大大减少。

![image-20210206124430941](https://i.loli.net/2021/02/06/vxC3HypjGFqLZze.png)

