---
title: 贝叶斯分类器
date: 2020-04-23 19:31:19
tags:
- 机器学习
---

## 写在前面

### 文明版

互联网的发展，可以让我等数学基础很差，也没有足够的学习的能力的学生也可以接触到机器学习，接触到贝叶斯分类器，实在是吾辈之幸，看不懂只能怪自己太菜了没本事

### 祖安版

周志华老师您写得那么晦涩让人看不懂显得很牛逼哈？

## 贝叶斯定理

不要公式，直接从例子入手。

考虑一个垃圾邮件分类器，训练集中，有25个垃圾邮件、75个非垃圾邮件，其中含有“Buy”这个关键字的分布如下：

![image-20200423193712325](https://i.loli.net/2020/04/23/fEkA6G3JtWy9qLT.png)

回答下面这个问题：

**考虑带有“Buy”关键字的邮件，有多大概率它是垃圾邮件？**
$$
P = \frac{20}{20+5}=80\%
$$
太简单了对不对？

这就是**贝叶斯定理**。

没了。

如果你非要看公式：

![image-20200423194203289](https://i.loli.net/2020/04/23/Bd5FXL2fQJOp3j6.png)

这就是公式，除了吓唬人没别的用。

## 朴素贝叶斯

还是刚才那个例子，如果我们不只考虑“buy”这一个关键字，还考虑，“cheap”这个词，那么问题变成什么样子呢？

![image-20200423194733606](https://i.loli.net/2020/04/23/JB8iYyEc5SqMCNU.png)

可能出现这样的现象，由于概率很小，有些取值在样本中没有找到，这也就是周志华老师说的“未被观测到”与“出现概率为零”通常是不同的。

有些问题可以用加大取样解决，但肯定有的不可以，那怎么办？

用概率算！

这里的**“朴素”**，就是假设“bug”和“cheap”这两个因素互相独立，更拽一点的说法是：

**假设所有属性相互独立**

那么，在“No spam”中，Buy 和 cheap 同时出现的样本，就可以表示为：

![image-20200423195206534](https://i.loli.net/2020/04/23/KVBYoqPZDJ5lH8y.png)

概率相乘就可以啦。

有了这个概率，我们还问刚才那个问题：

**考虑同时带有“Buy”和“cheap”关键字的邮件，有多大概率它是垃圾邮件？**

既然概率是0.5%，那“no spam”中同时包含两个字的样本就有$75*0.5\%=2/3$ 个，那下面就好算啦：
$$
P=\frac{12}{12+2/3}=94.737\%
$$
这，就是朴素贝叶斯分类器。

完事。

同样的，如果你还是很想看公式：

![image-20200423195901795](https://i.loli.net/2020/04/23/vKlRXxs1H5UkO8b.png)

这就是公式，除了吓唬人，没别的好看的。

## 术语

下面是一些关于朴素贝叶斯你要知道的术语，也是用来吓唬人的。

### 拉普拉斯修正

想想看，如果某个属性值没有在训练集中与某个类一起出现过，那么算起来就是0，相乘的时候，把别的有用的信息也给抹掉了，所以，不能让它是零。

那咋办嘛？

加个小点的数不就完了。

这就是拉普拉斯修正。

当然，如果你还是很想看看公式，OK

![image-20200423200359362](https://i.loli.net/2020/04/23/wFcqYZaji5kH42s.png)

### 先验概率、类条件概率（似然）、“证据”因子

其实用来吓唬人的话，我推荐用英语。

来，走起。

![image-20200423200929627](https://i.loli.net/2020/04/23/Q3pOU1dX65qm9bT.png)

#### 先验概率

prior，$P(c)$ 就是25%和75%，yes和no的概率，完了。

#### 类条件概率

class-conditional probability，也叫似然，likelihood，就是$P(x|c)$ 就是给定spam(yes)里面，有多少具有x这个样本的特征（带有“buy”和“cheep”关键字）的样本，完了。

#### 证据

evidence，就是$P(x)$ 就是yes和no所有样本里出现x这个特征的，说白了就是用来归一化的，后面求最大化它也没用，直接拿走了。完了。

### 下溢

就是乘得太多了，数变得太小，不好比。

伟大的数学家使用了**取对数** 这个方法，把连乘变成了连加，问题解决。完了。

### 贝叶斯网

因为 **朴素贝叶斯** 实在太 **朴素**，它的要求，**特征全部独立** 太苛刻，所以还是要考虑特征之间的约束关系，用一个DAG（有向无环图）来表示。

![image-20200423201729461](https://i.loli.net/2020/04/23/KLZroIncyWbxPgY.png)

爸爸决定儿子，参数代表概率。

用来吓唬人，这些差不多够了。

想真学知识，还是踏踏实实去啃西瓜书P156 - 162。（加油，你可以的O(∩_∩)O）

### EM算法

用来解决有些特征没有观测到的情况（隐变量）。

用来吓唬人，就背下面这句话：

![image-20200423203019213](https://i.loli.net/2020/04/23/kbYHlTf5OI4EuSW.png)

先计算期望，然后最大化，然后计算期望，然后最大化，然后期望，然后最大化。

你问我究竟是怎么回事？

这个西瓜书都没写，还是老老实实去看论文。

