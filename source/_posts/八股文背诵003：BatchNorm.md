---
title: 八股文背诵003：BatchNorm
date: 2021-10-24 21:42:55
tags:
- 机器学习

---

## Batchnorm

![image-20211024214525604](https://bat-blog.oss-cn-beijing.aliyuncs.com/image-20211024214525604.png)

之后就是找一个简单的白化，能过计算简单，同时保留信息。



所有引入 Batchnorm。

也就是：

1. 在X的每层，每个维度上作白化，也就不需要整体白化，计算复杂度大大降低。
2. 之后再跟一个linear，有一个α和β参数，保留了信息。

这些应付面试应该够了。

如果深入理解继续阅读：

https://zhuanlan.zhihu.com/p/34879333



## Warmup

一开始模型初始化，不能用太大的学习率，否则震荡过于强烈。



而模型在看过一些minibatch对数据有了一定理解之后，就可以适当增加学习率，适当的变化模型。



而当学习到了一定程度，就需要学习率慢慢变小（decay），因为此时模型已经慢慢收敛，过大的学习率有可能让模型崩掉或达到次优性能。
